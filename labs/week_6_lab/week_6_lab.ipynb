{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327fb514-0328-4e35-918a-1c0f4a98b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('equipment_tuning_dataset.csv')\n",
    "print(df.head())\n",
    "df.info(); df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1644ef-2488-4190-959d-a15aeb3b0551",
   "metadata": {},
   "source": [
    "# 1. Exploratory Data Analysis (EDA)\n",
    "Identify missing values and outliers, visualize distributions, and analyze class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d99eef-ef9d-49fa-8ddb-8b91a0067af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "df_numeric = df[[\"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F9\", \"F12\", \"Needs_Tuning\"]]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(df_numeric.columns):\n",
    "    sns.histplot(df_numeric[feature], ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61abd7e-28b6-4f27-8178-829a9c6816a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# analyze class balance\n",
    "df_cat = df[[\"F7\", \"F8\", \"F10\", \"F11\"]]\n",
    "\n",
    "# Plot category distribution\n",
    "fig_2, axes_2 = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes_2 = axes_2.flatten()\n",
    "for idx, feature in enumerate(df_cat.columns):\n",
    "    class_count_percent = df_cat[feature].value_counts() / len(df_cat) * 100\n",
    "    \n",
    "    sns.barplot(x=class_count_percent.index, y=class_count_percent.values, ax=axes_2[idx])\n",
    "    axes_2[idx].set_title(f'{feature} - Class Distribution')\n",
    "    axes_2[idx].set_xlabel('Class Value')\n",
    "    axes_2[idx].set_ylabel('Class Count Percentage (%)')\n",
    "    axes_2[idx].tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527f7c1-6ab2-4eca-b3df-5110f0a5b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values and outliers\n",
    "print(df.isna().mean()) # fraction missing per column\n",
    "\n",
    "# Outliers\n",
    "def iqr_mask(s, k=1.5):\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - k*iqr, q3 + k*iqr\n",
    "    return (s < lower) | (s > upper), (lower, upper)\n",
    "\n",
    "for c in [\"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F9\", \"F12\"]:\n",
    "    mask, (lo, hi) = iqr_mask(df[c])\n",
    "    print(f'{c}: {mask.sum()} outliers (IQR fences {lo:.2f}..{hi:.2f})')\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "df[[ 'F1', 'F2', 'F4', 'F6']].boxplot()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "df[[ 'F3', 'F5', 'F9', 'F12']].boxplot()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf5a73-794f-4c50-8708-ab321ca7c3b7",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Handle missing data, encode categorical variables, normalize or scale features, and justify preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b580d-88bb-4dff-9868-a91d8250d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# F3 and F9 having missing values which are MCAR, making KNN a suitable choice for imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "print(df.isna().mean())\n",
    "\n",
    "df_filled = df\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "cols_to_fill = ['F3', 'F9']\n",
    "df_filled[cols_to_fill] = imputer.fit_transform(df_filled[cols_to_fill])\n",
    "\n",
    "print(df_filled.isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520fe79-56ef-4990-8969-9c6b6eb732c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outliers\n",
    "df_outliers_present = df_filled.copy()\n",
    "df_outliers_winsored = df_filled.copy()\n",
    "df_outliers_removed = df_filled.copy()\n",
    "\n",
    "# Winsorize outliers\n",
    "for c in [\"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F9\", \"F12\"]:\n",
    "    mask, (lo, hi) = iqr_mask(df_outliers_winsored[c])\n",
    "    print(f'{c}: {mask.sum()} outliers (IQR fences {lo:.2f}..{hi:.2f})')\n",
    "    # Winsorize: clip to inner fences\n",
    "    df_outliers_winsored[c] = df_outliers_winsored[c].clip(lo, hi)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Trim outliers\n",
    "combined_mask = pd.Series(False, index=df_outliers_removed.index)\n",
    "\n",
    "for c in [\"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F9\", \"F12\"]:\n",
    "    mask, (lo, hi) = iqr_mask(df_outliers_removed[c])\n",
    "    print(f'{c}: {mask.sum()} outliers (IQR fences {lo:.2f}..{hi:.2f})')\n",
    "    combined_mask = combined_mask | mask\n",
    "\n",
    "# remove rows with outliers\n",
    "df_outliers_removed = df_outliers_removed[~combined_mask].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc31a10-7e73-46bc-8230-05fc17e3e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize or scale features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_outliers_present_numeric = df_outliers_present[[\"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F9\", \"F12\"]]\n",
    "df_outliers_winsored_numeric = df_outliers_winsored[[\"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F9\", \"F12\"]]\n",
    "df_outliers_removed_numeric = df_outliers_removed[[\"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F9\", \"F12\"]]\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# scale df with outliers present\n",
    "df_scaled_outliers_present = std_scaler.fit_transform(df_outliers_present_numeric.to_numpy())\n",
    "df_scaled_outliers_present = pd.DataFrame(df_scaled_outliers_present, columns=[\n",
    "  \"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F9\", \"F12\"])\n",
    "\n",
    "# scale df with outliers winsorized\n",
    "df_scaled_outliers_winsored = std_scaler.fit_transform(df_outliers_winsored_numeric.to_numpy())\n",
    "df_scaled_outliers_winsored = pd.DataFrame(df_scaled_outliers_winsored, columns=[\n",
    "  \"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F9\", \"F12\"])\n",
    "\n",
    "#scale df with outliers removed\n",
    "df_scaled_outliers_removed = std_scaler.fit_transform(df_outliers_removed_numeric.to_numpy())\n",
    "df_scaled_outliers_removed = pd.DataFrame(df_scaled_outliers_removed, columns=[\n",
    "  \"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F9\", \"F12\"])\n",
    "\n",
    "print(df_scaled_outliers_present.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95707885-9575-4430-8596-1ac19ebc6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_scaled_outliers_present_cat = df_outliers_present[[\"F7\", \"F8\", \"F10\", \"F11\"]]\n",
    "df_outliers_winsored_cat = df_outliers_winsored[[\"F7\", \"F8\", \"F10\", \"F11\"]]\n",
    "df_outliers_removed_cat = df_outliers_removed[[\"F7\", \"F8\", \"F10\", \"F11\"]]\n",
    "\n",
    "# print(df_outliers_winsored_cat.head(5))\n",
    "# print(df_scaled_outliers_winsored.head(5))\n",
    "\n",
    "df_combined_present = pd.concat([df_scaled_outliers_present, df_scaled_outliers_present_cat], axis=1)\n",
    "df_combined_winsored = pd.concat([df_scaled_outliers_winsored, df_outliers_winsored_cat], axis=1)\n",
    "df_combined_removed = pd.concat([df_scaled_outliers_removed, df_outliers_removed_cat], axis=1)\n",
    "\n",
    "# Outliers present\n",
    "X_1 = df_combined_present\n",
    "y_1 = df_outliers_present['Needs_Tuning']\n",
    "X_1 = pd.get_dummies(X_1, drop_first=True)\n",
    "\n",
    "# Outliers winsored\n",
    "X_2 = df_combined_winsored\n",
    "y_2 = df_outliers_winsored['Needs_Tuning']\n",
    "X_2 = pd.get_dummies(X_2, drop_first=True)\n",
    "\n",
    "#Outliers removed\n",
    "X_3 = df_combined_removed\n",
    "y_3 = df_outliers_removed['Needs_Tuning']\n",
    "X_3 = pd.get_dummies(X_3, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e2b26-4ea9-4f20-80b3-a2c15b211e10",
   "metadata": {},
   "source": [
    "# 3. Model Training: Implement and evaluate the following models:\n",
    "- k-Nearest Neighbors (kNN)\n",
    "- Na ̈ıve Bayes\n",
    "- Support Vector Machine (SVM)\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "- Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91002bf4-080c-4808-8020-627f86cd3c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models (fit/score pattern)\n",
    "# Model training WITHOUT test/train split for comparison to later split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "\n",
    "models = { \n",
    "    'KNN': Pipeline([('sc', StandardScaler(with_mean=False)), ('mdl', KNN())]),\n",
    "    'NB' : GaussianNB(),\n",
    "    'SVM': Pipeline([('sc', StandardScaler(with_mean=False)), ('mdl', SVC())]),\n",
    "    'DT' : DT(), \n",
    "    'RF' : RF(n_estimators=200, random_state=42),\n",
    "    'LR' : Pipeline([('sc', StandardScaler(with_mean=False)), ('mdl', LR(max_iter=1000))]),\n",
    "    'MLP': Pipeline([('sc', StandardScaler(with_mean=False)), ('mdl', MLP(max_iter=2000, random_state=42))]) \n",
    "}\n",
    "\n",
    "results_1_no_split = {}\n",
    "results_2_no_split = {}\n",
    "results_3_no_split = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_1, y_1)\n",
    "\n",
    "    y_pred_1 = model.predict(X_1)\n",
    "\n",
    "    acc_1 = accuracy_score(y_1, y_pred_1)\n",
    "\n",
    "    results_1_no_split[name] = round(acc_1, 4)  \n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_2, y_2)\n",
    "\n",
    "    y_pred_2 = model.predict(X_2)\n",
    "\n",
    "    acc_2 = accuracy_score(y_2, y_pred_2)\n",
    "\n",
    "    results_2_no_split[name] = round(acc_2, 4) \n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_3, y_3)\n",
    "\n",
    "    y_pred_3 = model.predict(X_3)\n",
    "\n",
    "    acc_3 = accuracy_score(y_3, y_pred_3)\n",
    "\n",
    "    results_3_no_split[name] = round(acc_3, 4)\n",
    "\n",
    "print(\"======No Test Train Split======\")\n",
    "print(f\"Outliers Present: {results_1_no_split}\")\n",
    "print(f\"Outliers Winsorised: {results_2_no_split}\")\n",
    "print(f\"Outliers Removed: {results_3_no_split}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de4f81-1ae3-48be-b102-1681ce11ca0b",
   "metadata": {},
   "source": [
    "# 4. Validation: Use both train/test split and 5-fold cross-validation to compare accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0e644-cd6b-4f92-af99-c614ad4f744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train with test/train split present for comparison\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.2, stratify=y_1,\n",
    "random_state=42)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.2, stratify=y_2,\n",
    "random_state=42)\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3, test_size=0.2, stratify=y_3,\n",
    "random_state=42)\n",
    "\n",
    "results_1_split = {}\n",
    "results_2_split = {}\n",
    "results_3_split = {}\n",
    "\n",
    "\n",
    "fig_1, axes_1 = plt.subplots(1, 7, figsize=(35, 4))\n",
    "axes_1 = axes_1.flatten()\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    model.fit(X_train_1, y_train_1)\n",
    "\n",
    "    y_pred_1 = model.predict(X_test_1)\n",
    "\n",
    "    acc_1 = accuracy_score(y_test_1, y_pred_1)\n",
    "\n",
    "    results_1_split[name] = round(acc_1, 4)  \n",
    "\n",
    "    disp_1 = ConfusionMatrixDisplay(confusion_matrix(y_test_1, y_pred_1),\n",
    "                                   display_labels=model.classes_)\n",
    "    # Plot on the correct subplot\n",
    "    disp_1.plot(ax=axes_1[i])\n",
    "    axes_1[i].set_title(f\"{name} Confusion Matrix\")\n",
    "plt.suptitle(\"Outliers Present\")\n",
    "\n",
    "fig_2, axes_2 = plt.subplots(1, 7, figsize=(35, 4))\n",
    "axes_2 = axes_2.flatten()\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    model.fit(X_train_2, y_train_2)\n",
    "\n",
    "    y_pred_2 = model.predict(X_test_2)\n",
    "\n",
    "    acc_2 = accuracy_score(y_test_2, y_pred_2)\n",
    "\n",
    "    results_2_split[name] = round(acc_2, 4) \n",
    "\n",
    "    disp_2 = ConfusionMatrixDisplay(confusion_matrix(y_test_2, y_pred_2),\n",
    "                                   display_labels=model.classes_)\n",
    "    # Plot on the correct subplot\n",
    "    disp_2.plot(ax=axes_2[i])\n",
    "    axes_2[i].set_title(f\"{name} Confusion Matrix\")\n",
    "plt.suptitle(\"Outliers Winsorized\")\n",
    "\n",
    "fig_3, axes_3 = plt.subplots(1, 7, figsize=(35, 4))\n",
    "axes_3 = axes_3.flatten()\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    model.fit(X_train_3, y_train_3)\n",
    "\n",
    "    y_pred_3 = model.predict(X_test_3)\n",
    "\n",
    "    acc_3 = accuracy_score(y_test_3, y_pred_3)\n",
    "\n",
    "    results_3_split[name] = round(acc_3, 4)\n",
    "\n",
    "    disp_3 = ConfusionMatrixDisplay(confusion_matrix(y_test_3, y_pred_3),\n",
    "                                   display_labels=model.classes_)\n",
    "    # Plot on the correct subplot\n",
    "    disp_3.plot(ax=axes_3[i])\n",
    "    axes_3[i].set_title(f\"{name} Confusion Matrix\")\n",
    "plt.suptitle(\"Outliers Trimed\")\n",
    "\n",
    "print(\"======Used Test Train Split======\")\n",
    "print(f\"Outliers Present: {results_1_split}\")\n",
    "print(f\"Outliers Winsorised: {results_2_split}\")\n",
    "print(f\"Outliers Removed: {results_3_split}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff00100-2041-430b-9503-d4fa1eaf6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation & Box Plot\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores_1 = {name: cross_val_score(mdl, X_1, y_1, cv=5, scoring='accuracy') for name, mdl in\n",
    "models.items()}\n",
    "scores_2 = {name: cross_val_score(mdl, X_2, y_2, cv=5, scoring='accuracy') for name, mdl in\n",
    "models.items()}\n",
    "scores_3 = {name: cross_val_score(mdl, X_3, y_3, cv=5, scoring='accuracy') for name, mdl in\n",
    "models.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e0492-3565-45d8-8a96-71d2a88be51c",
   "metadata": {},
   "source": [
    "# 5. Visualization: Create a box plot of accuracies for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776ca64-a393-44a4-b22c-0c1039f10a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.boxplot(scores_1.values(), tick_labels=scores_1.keys())\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('CV Accuracy by Model (Outliers Present)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot(scores_2.values(), tick_labels=scores_2.keys())\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('CV Accuracy by Model (Outliers Winsorised)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.boxplot(scores_3.values(), tick_labels=scores_3.keys())\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('CV Accuracy by Model (Outliers Removed)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80073d15-84e8-4d87-afaf-bb7dc125b92f",
   "metadata": {},
   "source": [
    "# 6. Discussion: Analyze which preprocessing and model choices led to higher accuracy, and discuss trade-offs in interpretability and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ee631-b739-40eb-92b8-c5e8f8eb0d31",
   "metadata": {},
   "source": [
    "- Pre-processing made a significant impact in two areas: outlier-handling and the test-train split\n",
    "- Models were trained with and without a test-train split; from the accuracy metrics of each of these models\n",
    "    - The models without a test-train split saw a much higher degree of accuracy,\n",
    "      this is likely due to overfitting from the lack of test-train split,\n",
    "      rather than a reflection of the actual accuracy of the model, as without the split, Random Forest and Decision Tree both achieve an accuracy of 1.0, or perfect accuracy, which is practically impossible\n",
    "- 3 forms of outlier-handling were implemented: no handling (outliers present), winsorisation, and trimming\n",
    "  these 3 methods were applied to both datasets with a test-train split and those without\n",
    "- Un-split dataset:\n",
    "    - accuracy was fairly similar across all three methods,\n",
    "    - trimming had a slightly higher KNN and MLP accuracy\n",
    "    - winsorisation had slightly higher accuracy in all other models\n",
    "    - this indicates some form of outlier handling improves accuracy over no handling\n",
    "- Split dataset:\n",
    "    - accuracy remained fairly similar acorss all 3 methids\n",
    "    - winsorisation showed the highest accuracy for KNN, SVM, DT, RF and LR\n",
    "    - no-handling had the highest accuracy for naive bayes\n",
    "    - trimming had the highest accuracy for logistic regression and MLP\n",
    "    - from these findings, it can be concluded that outliers are important to this problem and it is best not to use imputation in order to\n",
    "      maximize accuracy in this case, regardless of the test-train split\n",
    "    \n",
    "- The trade-off between accuracy and interpretability & complexity is a frequently\n",
    "  discussed issue in ML, although it dosen't appear to be highly relevant to the case at hand\n",
    "- Comparing the highest accuracy scores between inherently interpretable models (KNN, NB, DT, LR) and\n",
    "  non-inherently interpretable models (SVM, RF, MLP)\n",
    "  within the datasets which used a train-test split, there appears to be an even\n",
    "  disitribution of highest accuracy scores between interpretable and non-interpretable models\n",
    "    - the highest accuracy for non-handling is NB\n",
    "    - the highest accuracy for winsorising is RF\n",
    "    - the highest accuracy for trimming is logistic regression\n",
    "    - Overall highest accuracy was in the no-outlier-handling dataset,\n",
    "      with RF having the highest accuracy at 0.664 and NB having the second highest at 0.644\n",
    "    - The mix of interpretablility between NB, RF and LR shows that the trade-off isn't majorly significant in this case\n",
    "- Also, there was a low variance of accuracy scores across models, which indicates that there is very little trade off in accuracy\n",
    "  to interpretability when comparing between inherently interpretable vs. non-inherenrtly interpretable models\n",
    "\n",
    "- Ultimatley there will always be some degree of trade-off between interpretability and complexity, as generally,\n",
    "  as a concept in any domain becomes more complex, it becomes more difficult to understand.\n",
    "- On the contrary, non-inherently interpretable models like SVM or MLP can be made interpretable\n",
    "  with the assistance of tools like LIME or SHAP, which provide explanations of these models decisions, helping to mitigate the trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec2964a-2a77-46f8-8f08-3d5b2310208e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
